(window.webpackJsonp=window.webpackJsonp||[]).push([[28],{591:function(t,s,n){"use strict";n.r(s);var a=n(59),e=Object(a.a)({},(function(){var t=this,s=t.$createElement,n=t._self._c||s;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("h1",{attrs:{id:"react的diff算法"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#react的diff算法"}},[t._v("#")]),t._v(" react的diff算法")]),t._v(" "),n("h2",{attrs:{id:"起因"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#起因"}},[t._v("#")]),t._v(" 起因")]),t._v(" "),n("p",[n("code",[t._v("React")]),t._v("的"),n("code",[t._v("key")]),t._v("相信大家都很了解, 也不用我多说.但是平时对于"),n("code",[t._v("key")]),t._v("的使用可能没有那么严格, 有可能不给"),n("code",[t._v("key")]),t._v(", 有可能给"),n("code",[t._v("index")]),t._v(", 其实一般来说不会出什么问题, 顶多就是性能上会有一些损失, 但是在某些特定的情况下使用不当也可能会导致 "),n("code",[t._v("bug")]),t._v(", 比如下面这种情况.")]),t._v(" "),n("h2",{attrs:{id:"key导致的bug"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#key导致的bug"}},[t._v("#")]),t._v(" key导致的bug")]),t._v(" "),n("p",[t._v("在一个后台管理系统中, 左侧是一个菜单可以选择不同的选项, 右侧对应了不同的视频.但是左侧菜单切换的时候, 右侧视频的封面图虽然重新加载了, 点击播放后视频的内容并不是新的, 还是上一个选项卡的视频, 看起来就像没有重新渲染一样.而排查了代码之后发现我确实没有给"),n("code",[t._v("table")]),t._v("去加"),n("code",[t._v("key")]),t._v("值, 而加上这个"),n("code",[t._v("key")]),t._v("值之后也确实是好了.我们来看看"),n("code",[t._v("video")]),t._v("那段代码.")]),t._v(" "),n("div",{staticClass:"language-html extra-class"},[n("pre",{pre:!0,attrs:{class:"language-html"}},[n("code",[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("video")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token attr-name"}},[t._v("controls")]),n("span",{pre:!0,attrs:{class:"token attr-value"}},[n("span",{pre:!0,attrs:{class:"token punctuation attr-equals"}},[t._v("=")]),t._v("{true}")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token attr-name"}},[t._v("poster")]),n("span",{pre:!0,attrs:{class:"token attr-value"}},[n("span",{pre:!0,attrs:{class:"token punctuation attr-equals"}},[t._v("=")]),t._v("{poster}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("source")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token attr-name"}},[t._v("src")]),n("span",{pre:!0,attrs:{class:"token attr-value"}},[n("span",{pre:!0,attrs:{class:"token punctuation attr-equals"}},[t._v("=")]),t._v("{videoUrl}")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("/>")])]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("video")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n")])])]),n("p",[t._v("那么问题来了:")]),t._v(" "),n("ol",[n("li",[t._v("如果我不加"),n("code",[t._v("key")]),t._v(", "),n("code",[t._v("react")]),t._v("不是应该把各个菜单的内容理解为完全不一样而重新渲染吗?")]),t._v(" "),n("li",[n("code",[t._v("video")]),t._v("里面的"),n("code",[t._v("src")]),t._v("确实已经变了, 但是播放的视频还是老视频\n一顿搜索之后, 我先找到了第二个问题的答案:\n"),n("code",[t._v("video")]),t._v(" 用这种 "),n("code",[t._v("source")]),t._v(" 的写法时, 如果只改变 "),n("code",[t._v("src")]),t._v(" 的话, "),n("code",[t._v("video")]),t._v(" 是不会重新去加载视频的, 正确的做法应该是将 "),n("code",[t._v("dom")]),t._v(" 卸载掉再重新加载, 这样才能正确拉取到对应的视频.\n虽然我感觉这很不符合常识, 但结果就是这样的, 在不加 "),n("code",[t._v("key")]),t._v(" 的情况下, 去掉 "),n("code",[t._v("source")]),t._v(" 直接在 "),n("code",[t._v("video")]),t._v(" 中使用 "),n("code",[t._v("src")]),t._v(" 也能解决这个 "),n("code",[t._v("bug")]),t._v(".像下面这样")])]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("video controls"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" poster"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("poster"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" src"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("videoUrl"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n")])])]),n("p",[t._v("那么我们回到第一个问题, 我没有给"),n("code",[t._v("key")]),t._v("值, "),n("code",[t._v("React")]),t._v("居然不会把他们都认为是不一样的"),n("code",[t._v("dom")]),t._v("去重新渲染?")]),t._v(" "),n("h2",{attrs:{id:"diff算法"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#diff算法"}},[t._v("#")]),t._v(" diff算法")]),t._v(" "),n("p",[n("code",[t._v("key")]),t._v(" 值有什么用呢?在 "),n("code",[t._v("react")]),t._v(" 的 "),n("code",[t._v("diff")]),t._v(" 过程中, 如果遇见 "),n("code",[t._v("key")]),t._v(" 相同的两个结点, "),n("code",[t._v("react")]),t._v(" 会认为这是两个相同的结点, 在下一次渲染中会复用这个结点, 减少渲染的内容, 从而提升性能.那, "),n("code",[t._v("react")]),t._v(" 的 "),n("code",[t._v("diff")]),t._v(" 算法又是什么呢?")]),t._v(" "),n("h3",{attrs:{id:"传统的diff算法"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#传统的diff算法"}},[t._v("#")]),t._v(" 传统的diff算法")]),t._v(" "),n("p",[t._v("我没有仔细去研究过完全找出两颗树的改动之处最小的时间复杂度是多少, 根据网上的信息来看, 目前最小的时间复杂度也到了"),n("code",[t._v("O(n^3)")]),t._v(", 而"),n("code",[t._v("React")]),t._v("目前的"),n("code",[t._v("diff")]),t._v("算法时间复杂度为"),n("code",[t._v("O(n)")]),t._v(", 他们两个都不在一个量级上, 所以 "),n("code",[t._v("react")]),t._v(" 的 "),n("code",[t._v("diff")]),t._v(" 势必是丢掉了一些东西的.")]),t._v(" "),n("h3",{attrs:{id:"react的diff算法-2"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#react的diff算法-2"}},[t._v("#")]),t._v(" react的diff算法")]),t._v(" "),n("p",[t._v("为了降低算法复杂度, "),n("code",[t._v("React")]),t._v(" 针对前端开发的习惯做了一些限制:")]),t._v(" "),n("ol",[n("li",[n("code",[t._v("React")]),t._v(" 只会对同级元素 "),n("code",[t._v("diff")]),t._v(", 如果一个 "),n("code",[t._v("dom")]),t._v(" 结点跨越了层级, 那么它是永远不可能被复用的.")]),t._v(" "),n("li",[t._v("如果 "),n("code",[t._v("dom")]),t._v(" 结点的类型发生了改变, 这个结点以及其后代都会被销毁然后重新渲染")]),t._v(" "),n("li",[n("code",[t._v("react")]),t._v(" 可以通过 "),n("code",[t._v("key")]),t._v(" 值来判断哪些结点属于同一个结点")])]),t._v(" "),n("p",[t._v("所以通过这三条我们可以总结出要想让我们的页面少一些不必要的渲染, 我们可以:")]),t._v(" "),n("ol",[n("li",[t._v("不要轻易改变 "),n("code",[t._v("dom")]),t._v(" 层级")]),t._v(" "),n("li",[t._v("不要轻易改变 "),n("code",[t._v("dom")]),t._v(" 类型")]),t._v(" "),n("li",[t._v("利用好 "),n("code",[t._v("key")])])]),t._v(" "),n("h3",{attrs:{id:"diff实现"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#diff实现"}},[t._v("#")]),t._v(" diff实现")]),t._v(" "),n("p",[t._v("说了这么多, 那么我们就一起去看看 "),n("code",[t._v("react")]),t._v(" 到底是怎么实现 "),n("code",[t._v("diff")]),t._v(" 的, 而 "),n("code",[t._v("key")]),t._v(" 在这其中又发挥了怎样的作用")]),t._v(" "),n("h4",{attrs:{id:"入口函数"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#入口函数"}},[t._v("#")]),t._v(" 入口函数")]),t._v(" "),n("p",[n("code",[t._v("diff")]),t._v("的入口函数叫 "),n("code",[t._v("reconcileChildFibers")]),t._v(" , 在这之前经历了一系列的调度操作, 最后来到了diff环节, 那在"),n("code",[t._v("reconcileChildFibers")]),t._v("中 "),n("code",[t._v("react")]),t._v(" 又干了什么呢")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// returnFiber是我们最后将要渲染的Fiber树")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// currentFirstChild是当前的第一个子child")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// newChild是将要挂载的所有的子元素")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// lanes用于优先级判断")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("function")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("reconcileChildFibers")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token parameter"}},[t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" currentFirstChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 首先判断newChild是否是Fragment    ")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" isUnkeyedTopLevelFragment "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("typeof")]),t._v(" newChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'object'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&&")]),t._v(" newChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&&")]),t._v(" newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("type "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("REACT_FRAGMENT_TYPE")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&&")]),t._v(" newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 如果是Fragment则把里面的内容拿出来")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("isUnkeyedTopLevelFragment"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      newChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("props"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("children"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 判断newChild是不是对象")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" isObject "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("typeof")]),t._v(" newChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'object'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&&")]),t._v(" newChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 如果是对象则根据不同的类型去处理")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("isObject"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("switch")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("$$"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("typeof")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("case")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("REACT_ELEMENT_TYPE")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("placeSingleChild")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("reconcileSingleElement")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" currentFirstChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("case")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("REACT_PORTAL_TYPE")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("placeSingleChild")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("reconcileSinglePortal")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" currentFirstChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 判断是否是文本类型")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("typeof")]),t._v(" newChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'string'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("||")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("typeof")]),t._v(" newChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'number'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("placeSingleChild")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("reconcileSingleTextNode")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" currentFirstChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 判断是否是数组")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("isArray$1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("reconcileChildrenArray")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" currentFirstChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 判断是否可迭代")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("getIteratorFn")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("reconcileChildrenIterator")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" currentFirstChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 后面都是一些错误处理")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("isObject"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("throwOnInvalidObjectType")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("typeof")]),t._v(" newChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'function'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("warnOnFunctionType")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("typeof")]),t._v(" newChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'undefined'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&&")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!")]),t._v("isUnkeyedTopLevelFragment"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// If the new child is undefined, and the return fiber is a composite")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// component, throw an error. If Fiber return types are disabled,")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// we already threw above.")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("switch")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tag"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("case")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token literal-property property"}},[t._v("ClassComponent")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n              "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" instance "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stateNode"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n              "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("instance"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("render"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_isMockFunction"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// We allow auto-mocks to proceed as if they're returning null.")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("break")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n              "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Intentionally fall through to the next case, which handles both")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// functions and classes")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// eslint-disable-next-lined no-fallthrough")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("case")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token literal-property property"}},[t._v("Block")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("case")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token literal-property property"}},[t._v("FunctionComponent")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("case")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token literal-property property"}},[t._v("ForwardRef")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("case")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token literal-property property"}},[t._v("SimpleMemoComponent")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n              "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throw")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("Error")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("getComponentName")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("type"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("||")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Component'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"(...): Nothing was returned from render. This usually means a return statement is missing. Or, to render nothing, return null."')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n              "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Remaining cases are all treated as empty.")]),t._v("\n\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("deleteRemainingChildren")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" currentFirstChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("p",[t._v("可以看到, 这个入口函数相对于一个路由文件, 将不同的类型分派到不同的子函数去处理.那么 "),n("code",[t._v("React")]),t._v(" 会如何处理各种类型的结点了, 我们接着看.")]),t._v(" "),n("h4",{attrs:{id:"同级单个结点diff"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#同级单个结点diff"}},[t._v("#")]),t._v(" 同级单个结点diff")]),t._v(" "),n("p",[t._v("如果同级只有一个结点, 那么会进入 "),n("code",[t._v("reconcileSingleElement")]),t._v(" 中去处理, 我们来看看这里面做了什么操作.")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("function")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("reconcileSingleElement")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token parameter"}},[t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" currentFirstChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n   "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n   "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" child "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" currentFirstChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n   "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 首先会判断之前有没有这个结点")]),t._v("\n   "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("child "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n     "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 有老结点的情况下判断key")]),t._v("\n     "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("child"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n       "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 根据tag类型来处理")]),t._v("\n       "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("switch")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("child"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tag"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n         "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("case")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token literal-property property"}},[t._v("Fragment")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\n           "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n             "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("type "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("REACT_FRAGMENT_TYPE")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n               "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("deleteRemainingChildren")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" child"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sibling"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n               "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" existing "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("useFiber")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("child"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("props"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("children"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n               existing"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("return "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n               "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                 existing"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_debugSource "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_source"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n                 existing"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_debugOwner "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_owner"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n               "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n               "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" existing"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n             "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n             "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("break")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n           "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n         "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("case")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token literal-property property"}},[t._v("Block")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\n\n         "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// We intentionally fallthrough here if enableBlocksAPI is not on.")]),t._v("\n         "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// eslint-disable-next-lined no-fallthrough")]),t._v("\n\n         "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("default")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\n           "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n             "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("child"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("elementType "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("type "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("||")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Keep this check inline so it only runs on the false path:")]),t._v("\n               "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("isCompatibleFamilyForHotReloading")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("child"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n               "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("deleteRemainingChildren")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" child"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sibling"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n               "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 复用当前结点")]),t._v("\n               "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" _existing3 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("useFiber")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("child"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("props"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n               _existing3"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ref "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("coerceRef")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" child"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n               _existing3"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("return "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n               "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                 _existing3"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_debugSource "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_source"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n                 _existing3"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_debugOwner "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_owner"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n               "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n               "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" _existing3"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n             "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n             "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("break")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n           "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n       "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Didn't match.")]),t._v("\n\n\n       "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("deleteRemainingChildren")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" child"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n       "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("break")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n     "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n       "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("deleteChild")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" child"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n     "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n     child "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" child"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sibling"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n   "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n   "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 没有老结点只能创建新结点")]),t._v("\n   "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("type "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("REACT_FRAGMENT_TYPE")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n     "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" created "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("createFiberFromFragment")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("props"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("children"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mode"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n     created"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("return "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n     "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" created"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n   "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n     "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" _created4 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("createFiberFromElement")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mode"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n     _created4"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ref "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("coerceRef")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" currentFirstChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n     _created4"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("return "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n     "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" _created4"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n   "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("p",[t._v("可以看到, 首先会判断之前是否存在这个结点, 如果之前都不存在, 那复用更谈不上了, 所以会直接新建一个插入.\n而如果存在老结点, 下面会判断 "),n("code",[t._v("key")]),t._v(" 和 "),n("code",[t._v("type")]),t._v("是否相同.注意, 在 "),n("code",[t._v("key")]),t._v(" 相同之后, 只有 "),n("code",[t._v("type")]),t._v(" 也相同的情况下 "),n("code",[t._v("react")]),t._v(" 才会去复用这个结点.而如果判断出 "),n("code",[t._v("key")]),t._v(" 或者 "),n("code",[t._v("type")]),t._v(" 不同, "),n("code",[t._v("react")]),t._v(" 会删掉这个结点, 去新建一个结点并返回.")]),t._v(" "),n("h4",{attrs:{id:"同级多个结点diff"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#同级多个结点diff"}},[t._v("#")]),t._v(" 同级多个结点diff")]),t._v(" "),n("p",[t._v("同级多个结点的"),n("code",[t._v("diff")]),t._v("就比单个结点要复杂多了, 它会进入"),n("code",[t._v("reconcileChildrenArray")]),t._v("中去处理, 我们慢慢来看.")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("function")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("reconcileChildrenArray")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token parameter"}},[t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" currentFirstChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 判断key, 如果这里出现相同的key会给一个warning")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" knownKeys "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" i "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" i "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" newChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("length"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" i"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("++")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" child "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" newChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        knownKeys "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("warnOnInvalidKey")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("child"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" knownKeys"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// diff后的结果")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" resultingFirstChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 新fiber树的上一个fiberNode")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" previousNewFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 老fiber树的fiberNode, 也就是未更新之前")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" oldFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" currentFirstChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 当前可复用的结点的index")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" lastPlacedIndex "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 新child的index")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" newIdx "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 下一个老的fiberNode")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" nextOldFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 这里用newIndex来遍历")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" oldFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&&")]),t._v(" newIdx "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" newChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("length"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" newIdx"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("++")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 没想到有什么情况会进第一个分支")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("oldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" newIdx"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        nextOldFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" oldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        oldFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// sibling是fiberNode的兄弟结点, 所以用nextOldFiber来存储下一个需要比较的fiberNode")]),t._v("\n        nextOldFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" oldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sibling"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 这里通过比较oldFiber和newFiber来拿到一个新的fiberNode对象")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 这个对象可能是复用的、新建的、也可能为null")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" newFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("updateSlot")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" oldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("newIdx"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 如果为null的情况下直接break,跳出循环")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("newFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("oldFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          oldFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nextOldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("break")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 优化项, 不用管")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("shouldTrackSideEffects"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("oldFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&&")]),t._v(" newFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("alternate "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// We matched the slot, but we didn't reuse the existing fiber, so we")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// need to delete the existing child.")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("deleteChild")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" oldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 记录下当前可复用结点的index")]),t._v("\n      lastPlacedIndex "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("placeChild")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("newFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lastPlacedIndex"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newIdx"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 是否是最开始的结点")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("previousNewFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        resultingFirstChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" newFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 设置兄弟结点为newFiber")]),t._v("\n        previousNewFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sibling "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" newFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 链表移动, pre赋值为当前node")]),t._v("\n      previousNewFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" newFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 将old赋值为下一个需要比较的对象")]),t._v("\n      oldFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nextOldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 如果是newIdx为length跳出的循环, 证明newChild已经遍历完了")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 那么此时剩下的oldFiber一定是被删除的结点, 直接删除")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("newIdx "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" newChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("length"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("deleteRemainingChildren")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" oldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" resultingFirstChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 如果oldFiber为null了, 那么证明老的fiber已经遍历完了")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 那么剩下的newChild都是新增的结点, 直接插入")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("oldFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" newIdx "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" newChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("length"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" newIdx"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("++")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" _newFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("createChild")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("newIdx"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_newFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("continue")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n        lastPlacedIndex "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("placeChild")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_newFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lastPlacedIndex"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newIdx"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("previousNewFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          resultingFirstChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" _newFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          previousNewFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sibling "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" _newFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n        previousNewFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" _newFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" resultingFirstChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" \n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 如果是中途break, 证明出现了结点变化")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 此时收集未遍历的oldFiber, 以key作为键值, map结构存储")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" existingChildren "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("mapRemainingChildren")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" oldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Keep scanning and use the map to restore deleted items as moves.")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 遍历newChild, 用key去existingChildren中寻找对应的结点")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" newIdx "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" newChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("length"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" newIdx"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("++")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" _newFiber2 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("updateFromMap")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("existingChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newIdx"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("newIdx"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_newFiber2 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("shouldTrackSideEffects"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_newFiber2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("alternate "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            existingChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("delete")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_newFiber2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("?")]),t._v(" newIdx "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" _newFiber2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 找到了之后给effectTag赋值")]),t._v("\n        lastPlacedIndex "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("placeChild")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_newFiber2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lastPlacedIndex"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newIdx"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("previousNewFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          resultingFirstChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" _newFiber2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          previousNewFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sibling "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" _newFiber2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n        previousNewFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" _newFiber2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("shouldTrackSideEffects"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Any existing children that weren't consumed above were deleted. We need")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// to add them to the deletion list.")]),t._v("\n      existingChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("forEach")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("function")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token parameter"}},[t._v("child")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("deleteChild")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" child"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 返回最后的结果")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" resultingFirstChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("p",[t._v("首先, 一进来, "),n("code",[t._v("react")]),t._v(" 就会检查一遍各个子结点上的 "),n("code",[t._v("key")]),t._v(" 值.在"),n("code",[t._v("warnOnInvalidKey")]),t._v("中, 用了 "),n("code",[t._v("Set")]),t._v(" 来存储 "),n("code",[t._v("key")]),t._v(" 值, 如果发现了两个一样的值, 就会抛出我们平常见的挺多的一个错误了:")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token function"}},[t._v("error")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Encountered two children with the same key, `%s`. '")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Keys should be unique so that components maintain their identity '")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'across updates. Non-unique keys may cause children to be '")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'duplicated and/or omitted — the behavior is unsupported and '")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'could change in a future version.'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),n("p",[t._v("后面, 就是真正的 "),n("code",[t._v("diff")]),t._v(" 逻辑了, 我先给大家解释一下"),n("code",[t._v("react")]),t._v("是如何"),n("code",[t._v("diff")]),t._v("的.")]),t._v(" "),n("ol",[n("li",[t._v("遍历新老 "),n("code",[t._v("child")]),t._v(" 并进行对比, 如果 "),n("code",[t._v("key")]),t._v(", "),n("code",[t._v("type")]),t._v(" 全都能对上, 那最好了, 这些结点就能完全复用, "),n("code",[t._v("diff")]),t._v("结束")]),t._v(" "),n("li",[t._v("中途某个结点被改变了, 导致遍历中断, 这时候 "),n("code",[t._v("react")]),t._v(" 会用 "),n("code",[t._v("Map")]),t._v(" 以 "),n("code",[t._v("key")]),t._v(" 为键去收集 "),n("code",[t._v("oldChildren")]),t._v("（即老结点）中没有被遍历到的结点, 随后遍历剩余新结点.这个时候 "),n("code",[t._v("key")]),t._v(" 就发挥了作用, 遍历新结点的过程中, 能不能复用老结点就是通过能不能找到 "),n("code",[t._v("key")]),t._v(" 来判断的, 如果在 "),n("code",[t._v("map")]),t._v(" 中找到了, 那么就复用, 如果找不到, 那么就新建.")]),t._v(" "),n("li",[n("code",[t._v("oldChild")]),t._v(" 被遍历完了, "),n("code",[t._v("newChild")]),t._v(" 中还有结点, 那么证明这些结点是被插入的, 直接插入.")]),t._v(" "),n("li",[n("code",[t._v("newChild")]),t._v(" 被遍历完了, "),n("code",[t._v("oldChild")]),t._v(" 还有结点, 那么证明这些结点是被删除的, 直接删除.\n下面我们结合代码来看:")])]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[t._v("    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 这里用newIndex来遍历")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" oldFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&&")]),t._v(" newIdx "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" newChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("length"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" newIdx"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("++")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 没想到有什么情况会进第一个分支")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("oldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" newIdx"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        nextOldFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" oldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        oldFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// sibling是fiberNode的兄弟结点, 所以用nextOldFiber来存储下一个需要比较的fiberNode")]),t._v("\n        nextOldFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" oldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sibling"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 这里通过比较oldFiber和newFiber来拿到一个新的fiberNode对象")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 这个对象可能是复用的、新建的、也可能为null")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" newFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("updateSlot")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" oldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("newIdx"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 如果为null的情况下直接break,跳出循环")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("newFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("oldFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          oldFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nextOldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("break")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 优化项, 不用管")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("shouldTrackSideEffects"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("oldFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&&")]),t._v(" newFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("alternate "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// We matched the slot, but we didn't reuse the existing fiber, so we")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// need to delete the existing child.")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("deleteChild")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" oldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 记录下当前可复用结点的index")]),t._v("\n      lastPlacedIndex "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("placeChild")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("newFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lastPlacedIndex"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newIdx"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 是否是最开始的结点")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("previousNewFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        resultingFirstChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" newFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 设置兄弟结点为newFiber")]),t._v("\n        previousNewFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sibling "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" newFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 链表移动, pre赋值为当前node")]),t._v("\n      previousNewFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" newFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 将old赋值为下一个需要比较的对象")]),t._v("\n      oldFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nextOldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("p",[t._v("这里是 "),n("code",[t._v("newIndex++")]),t._v(", 所以是在 "),n("code",[t._v("for")]),t._v(" 循环遍历 "),n("code",[t._v("newChild")]),t._v(", 首先它会比较 "),n("code",[t._v("oldFiber")]),t._v(" 的 "),n("code",[t._v("index")]),t._v(" 和 "),n("code",[t._v("newIndex")]),t._v(" 的大小, 因为他们都是从头开始遍历的, 所以正常情况下他们肯定是相等的, 也就是会走到 "),n("code",[t._v("else")]),t._v(" 分支, 让 "),n("code",[t._v("nextOldFiber")]),t._v(" 等于 "),n("code",[t._v("oldFiber")]),t._v(" 的兄弟结点, 也就是当前结点的相邻结点.然后 "),n("code",[t._v("newFiber")]),t._v(" 是通过"),n("code",[t._v("updateSlot")]),t._v(" 得到的, 我们进去这个函数里面看看")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("function")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("updateSlot")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token parameter"}},[t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" oldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 拿到key值, 没有则为null")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" oldFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("?")]),t._v(" oldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("typeof")]),t._v(" newChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'string'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("||")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("typeof")]),t._v(" newChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'number'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("updateTextNode")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" oldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("typeof")]),t._v(" newChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'object'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&&")]),t._v(" newChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("switch")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("$$"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("typeof")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("case")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("REACT_ELEMENT_TYPE")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 都没有给key都为null, 则也是相等, 可以复用")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n              "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("type "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("REACT_FRAGMENT_TYPE")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("updateFragment")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" oldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("props"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("children"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n              "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n              "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 得到更新后的Element")]),t._v("\n              "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("updateElement")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" oldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n              "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("case")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token constant"}},[t._v("REACT_PORTAL_TYPE")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n              "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("updatePortal")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" oldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n              "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("isArray$1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("||")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("getIteratorFn")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("updateFragment")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" oldFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n      "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("throwOnInvalidObjectType")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("typeof")]),t._v(" newChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'function'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("warnOnFunctionType")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 从这里面可以看到, 只有 key 相同的情况下, 才会返回一个 fiber, 否则会返回 null.我们先来看 key 相同的情况下, `updateElement`做了什么")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("function")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("updateElement")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token parameter"}},[t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" current"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 判断是否有老结点")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("current "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 只有type相同才能复用")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("current"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("elementType "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("type "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("||")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" \n        "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("isCompatibleFamilyForHotReloading")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("current"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("x\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" existing "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("useFiber")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("current"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("props"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        existing"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ref "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("coerceRef")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" current"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        existing"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("return "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          existing"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_debugSource "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_source"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n          existing"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_debugOwner "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_owner"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" existing"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 没有老结点或者type不同直接创建一个新的结点")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" created "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("createFiberFromElement")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mode"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    created"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ref "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("coerceRef")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" current"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" element"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    created"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("return "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" created"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("p",[t._v("可以看到, 如果有老结点, 并且他们的 "),n("code",[t._v("type")]),t._v(" 相同, 会复用该结点, 反之则会创建一个新结点返回.")]),t._v(" "),n("p",[t._v("回到我们之前的循环中, 如果新老结点的 "),n("code",[t._v("key")]),t._v(" 相同, 那么才会返回一个 "),n("code",[t._v("fiber")]),t._v(" 结点.而如果我们不给 "),n("code",[t._v("key")]),t._v("呢?如果两个都不给 "),n("code",[t._v("key")]),t._v(" 的话, 其实就是 "),n("code",[t._v("null === null")]),t._v(", 最后的结果还是 "),n("code",[t._v("true")]),t._v(", 所以也会复用.而如果 "),n("code",[t._v("key")]),t._v(" 不一样, 那么返回了 "),n("code",[t._v("null")]),t._v(", 我们的 "),n("code",[t._v("for")]),t._v(" 循环就会 "),n("code",[t._v("break")]),t._v(" 了, 进入下一个逻辑, 这里我们后面再说.")]),t._v(" "),n("p",[t._v("在拿到新的 "),n("code",[t._v("fiberNode")]),t._v(" 之后, 此时会进入一个"),n("code",[t._v("placeChild")]),t._v("函数.他的作用主要是判断各个结点的更新类型并返回最后一个可复用结点的位置.")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("function")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("placeChild")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token parameter"}},[t._v("newFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lastPlacedIndex"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newIndex")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("//lastPlacedIndex初始值为0")]),t._v("\n    newFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" newIndex"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!")]),t._v("shouldTrackSideEffects"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" lastPlacedIndex"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" current "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" newFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("alternate"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 判断当前有没有结点")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("current "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" oldIndex "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" current"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 如果oldIndex小于最近可复用结点的index, 那么它需要右移")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("oldIndex "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" lastPlacedIndex"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Placement为一个常量, react用于判断更改类型")]),t._v("\n        newFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flags "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Placement"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" lastPlacedIndex"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 如果它比最近可复用结点大, 那么不需要改动, 同时把lastPlacedIndex设置为该值")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" oldIndex"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("//这个fiber是新建的")]),t._v("\n      newFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flags "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Placement"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" lastPlacedIndex"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n")])])]),n("p",[t._v("这个地方有点绕, 用大白话描述就是"),n("code",[t._v("react")]),t._v("会找到一批相对位置不变的节点, 保留他们的位置而移动其他节点. 我们用个简单的例子来理解, 现在有 "),n("code",[t._v("abcd")]),t._v(" 四个数, 我们要把它变成 "),n("code",[t._v("dabc")]),t._v(".如果我们肉眼来看, 那很显然是把 "),n("code",[t._v("d")]),t._v(" 移动到最前面, 但是 "),n("code",[t._v("react")]),t._v(" 不是这样干的, "),n("code",[t._v("react")]),t._v(" 是这样干的.")]),t._v(" "),n("ol",[n("li",[t._v("遍历"),n("code",[t._v("dabc")]),t._v(", 拿到第一个"),n("code",[t._v("d")]),t._v(", "),n("code",[t._v("d")]),t._v("在"),n("code",[t._v("abcd")]),t._v("中为第四个数, 所以"),n("code",[t._v("index=3")]),t._v(", 比"),n("code",[t._v("lastPlacedIndex=0")]),t._v("大, 位置保持不变, 并修改"),n("code",[t._v("lastPlacedIndex")]),t._v("为"),n("code",[t._v("3")])]),t._v(" "),n("li",[t._v("遍历"),n("code",[t._v("dabc")]),t._v(", 拿到第二个"),n("code",[t._v("a")]),t._v(", "),n("code",[t._v("a")]),t._v("在"),n("code",[t._v("abcd")]),t._v("中为第一个数, 所以"),n("code",[t._v("index=0")]),t._v(", 小于"),n("code",[t._v("lastPlacedIndex=3")]),t._v(", 为了保保证顺序, 需要把"),n("code",[t._v("a")]),t._v("右移到"),n("code",[t._v("d")]),t._v("后面去")]),t._v(" "),n("li",[t._v("遍历"),n("code",[t._v("dabc")]),t._v(", 拿到第三个"),n("code",[t._v("b")]),t._v(", "),n("code",[t._v("b")]),t._v("在"),n("code",[t._v("abcd")]),t._v("中为第二个数, 所以"),n("code",[t._v("index=1")]),t._v(", 小于"),n("code",[t._v("lastPlacedIndex=3")]),t._v(", 为了保证顺序, 需要把"),n("code",[t._v("b")]),t._v("右移到"),n("code",[t._v("d")]),t._v("后面去")]),t._v(" "),n("li",[t._v("遍历"),n("code",[t._v("dabc")]),t._v(", 拿到第三个"),n("code",[t._v("c")]),t._v(", "),n("code",[t._v("c")]),t._v("在"),n("code",[t._v("abcd")]),t._v("中为第一个数, 所以"),n("code",[t._v("index=2")]),t._v(", 小于"),n("code",[t._v("lastPlacedIndex=3")]),t._v(", 为了保证顺序, 需要把"),n("code",[t._v("c")]),t._v("右移到"),n("code",[t._v("d")]),t._v("后面去\n所以 "),n("code",[t._v("react")]),t._v(" 其实是把 "),n("code",[t._v("abc")]),t._v(" 移动到了"),n("code",[t._v("d")]),t._v(" 后面去, 当然换一种方法也是可以的, 但是对于"),n("code",[t._v("react")]),t._v("来说, 不可能根据实际情况去判断如何移动, 所以只会采取一种固定的方法来进行修改.")])]),t._v(" "),n("p",[t._v("我们接着看后面的代码, 如果"),n("code",[t._v("newIndex")]),t._v("等于"),n("code",[t._v("newChild")]),t._v("的长度了, 那么证明我们已经把"),n("code",[t._v("newChild")]),t._v("遍历完了.但是此时"),n("code",[t._v("oldChild")]),t._v("还有剩余, 那就证明我们这次操作删除了某些结点, 所以直接把剩下的结点删掉.")]),t._v(" "),n("p",[t._v("而如果"),n("code",[t._v("oldChild")]),t._v("已经遍历完了, 但是"),n("code",[t._v("newChild")]),t._v("还有, 那么证明我们添加了结点, 此时我们直接添加剩余结点即可.")]),t._v(" "),n("p",[t._v("但是如果我们中途退出了循环, 则证明我们修改了中途的结点, 此时会通过一个"),n("code",[t._v("mapRemainingChildren")]),t._v("函数去收集没有遍历完的老结点, 用"),n("code",[t._v("Map")]),t._v("以"),n("code",[t._v("key")]),t._v("作为键去存储, 如果没有"),n("code",[t._v("key")]),t._v("则用"),n("code",[t._v("index")]),t._v("当"),n("code",[t._v("key")]),t._v(".")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[t._v("  "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("function")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("mapRemainingChildren")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token parameter"}},[t._v("returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" currentFirstChild")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 新建一个map")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" existingChildren "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Map")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 拿到第一个fiber")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" existingChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" currentFirstChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 遍历")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("existingChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("existingChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 有key则用key")]),t._v("\n        existingChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("set")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("existingChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" existingChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 没有则用index")]),t._v("\n        existingChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("set")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("existingChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" existingChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 获取兄弟fiber")]),t._v("\n      existingChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" existingChild"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sibling"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" existingChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("p",[t._v("随后再次循环剩下的"),n("code",[t._v("newChild")]),t._v(", 去找其中对应的结点, 并通过"),n("code",[t._v("placeChild")]),t._v("函数标记我们需要做的操作.")]),t._v(" "),n("div",{staticClass:"language-js extra-class"},[n("pre",{pre:!0,attrs:{class:"language-js"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" newIdx "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" newChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("length"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" newIdx"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("++")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 这是根据之前的map找到的结点")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" _newFiber2 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("updateFromMap")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("existingChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" returnFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newIdx"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("newIdx"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lanes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n      "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_newFiber2 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("shouldTrackSideEffects"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_newFiber2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("alternate "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            existingChildren"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("delete")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_newFiber2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("?")]),t._v(" newIdx "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" _newFiber2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 标记结点修改类型")]),t._v("\n        lastPlacedIndex "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("placeChild")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_newFiber2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lastPlacedIndex"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" newIdx"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("previousNewFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("===")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("null")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          resultingFirstChild "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" _newFiber2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          previousNewFiber"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sibling "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" _newFiber2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 链表移动")]),t._v("\n        previousNewFiber "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" _newFiber2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("p",[t._v("这里有一个"),n("code",[t._v("updateFromMap")]),t._v(", 这里主要就是从之前的"),n("code",[t._v("map")]),t._v("中利用"),n("code",[t._v("key")]),t._v("来匹配对应的结点, 如果匹配到了则可以复用, 匹配不到就创建一个新的结点.")]),t._v(" "),n("h2",{attrs:{id:"总结"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[t._v("#")]),t._v(" 总结")]),t._v(" "),n("p",[t._v("其实, 最重要的几个点就是:")]),t._v(" "),n("ol",[n("li",[n("code",[t._v("key")]),t._v("值不给的情况下, "),n("code",[t._v("react")]),t._v("会利用"),n("code",[t._v("index")]),t._v("来做判断, 并不会粗暴的舍弃所有"),n("code",[t._v("dom")])]),t._v(" "),n("li",[t._v("只有"),n("code",[t._v("key")]),t._v("和"),n("code",[t._v("type")]),t._v("都相同的情况下, "),n("code",[t._v("react")]),t._v("才会去复用结点")]),t._v(" "),n("li",[t._v("改变"),n("code",[t._v("list")]),t._v("顺序时, "),n("code",[t._v("react")]),t._v("是通过从上往下移的顺序去改变的, 所以我们尽可能少把后面的结点移动到前面, 因为这其实会导致该结点前面的结点全部移动到后面, 并不是单纯的把这一个结点提到前面.")])]),t._v(" "),n("p",[t._v("最后, 我们的整个"),n("code",[t._v("diff")]),t._v("过程就走完了.我们再次回到最初的问题, 如果我不给"),n("code",[t._v("key")]),t._v("值, "),n("code",[t._v("react")]),t._v("不会把他们完全扔掉再构造吗?答案是不会."),n("code",[t._v("react")]),t._v("会用"),n("code",[t._v("index")]),t._v("当作"),n("code",[t._v("key")]),t._v("值, 而此时结点类型也没有改变, 所以"),n("code",[t._v("react")]),t._v("会复用该结点, 只是改变了他的"),n("code",[t._v("props")]),t._v(", 也就导致了"),n("code",[t._v("video没")]),t._v("有取得正确的视频源.最后我们再来总结一般整个流程:")]),t._v(" "),n("ol",[n("li",[t._v("进入入口函数以后根据不同的类型去处理")]),t._v(" "),n("li",[t._v("如果是"),n("code",[t._v("element")]),t._v("那么直接就可以更新了, 此时会根据"),n("code",[t._v("key")]),t._v("和"),n("code",[t._v("type")]),t._v("判断是否可以复用（都不传"),n("code",[t._v("key")]),t._v(", "),n("code",[t._v("key")]),t._v("也是相等）")]),t._v(" "),n("li",[t._v("如果是数组则进行特殊处理")]),t._v(" "),n("li",[t._v("首先循环一对一比较"),n("code",[t._v("oldFiber")]),t._v("和"),n("code",[t._v("newChild")]),t._v(", 如果完全一样那么都可以复用, 顺利结束")]),t._v(" "),n("li",[t._v("如果"),n("code",[t._v("oldFiber")]),t._v("遍历完了"),n("code",[t._v("newChild")]),t._v("还有, 那么是新加了结点, 直接把剩下的结点插入")]),t._v(" "),n("li",[t._v("如果"),n("code",[t._v("newChild")]),t._v("遍历完了"),n("code",[t._v("oldFiber")]),t._v("还有, 那么是删除了结点, 直接把剩下的结点删除")]),t._v(" "),n("li",[t._v("如果是中途退出了循环, 那么证明有结点被改变了, 此时收集剩下的"),n("code",[t._v("oldFiber")]),t._v(", 用"),n("code",[t._v("Map")]),t._v("存储, 以"),n("code",[t._v("key")]),t._v("为键值, 如果没有"),n("code",[t._v("key")]),t._v("则用"),n("code",[t._v("index")])]),t._v(" "),n("li",[t._v("遍历剩下的"),n("code",[t._v("newChild")]),t._v(", 在"),n("code",[t._v("map")]),t._v("中找对应的可复用结点, 找到了之后比较"),n("code",[t._v("type")]),t._v(", 相同则能复用.如果找不到或者"),n("code",[t._v("type")]),t._v("不同, 则创建新的结点")]),t._v(" "),n("li",[t._v("最后返回"),n("code",[t._v("diff")]),t._v("后的"),n("code",[t._v("fiber")]),t._v("\n这里我们只梳理了核心流程, 你可以注意到源码中其实还有一些其他类型的比较和一些优化措施, 这些就留给大家自己摸索了.")])])])}),[],!1,null,null,null);s.default=e.exports}}]);